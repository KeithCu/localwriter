{% extends "base.html" %}

{% block title %}Running Local AI Models{% endblock %}

{% block content %}
<section class="section">
    <div class="wrap prose">
        <h1>Running Local AI Models on Your Computer</h1>
        <p>LocalWriter doesn't run the AI models itselfâ€”it communicates with them. For the best privacy and control, you
            can run AI models directly on your own computer. This means no data leaves your machine and there are no
            monthly subscription fees.</p>
        <p>It might sound intimidating, but it's gotten incredibly easy. You don't need to be a software engineer to get
            a local AI running. Here are three fantastic, free tools that handle everything for you.</p>

        <h2>1. llama.cpp</h2>
        <p><strong>Best for:</strong> Maximum performance and control on older hardware.</p>
        <p><code>llama.cpp</code> is the under-the-hood engine that powers many other tools. It's incredibly efficient
            and runs well even on computers without powerful graphics cards. While it requires slightly more comfort
            with downloading files and using the command line, it's a fantastic, lightweight, free and open-source
            (FOSS) option.</p>
        <p>ðŸ‘‰ <a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener">Get llama.cpp</a></p>

        <h2>2. Ollama</h2>
        <p><strong>Best for:</strong> A dead-simple installation that runs quietly in the background.</p>
        <p>Ollama is extremely popular for its simplicity. While it primarily runs from the command line
            (Terminal/Prompt), it only takes one command to get a model running (e.g., <code>ollama run llama3</code>).
            Once running, it's instantly available to LocalWriter.</p>
        <p>ðŸ‘‰ <a href="https://ollama.com" target="_blank" rel="noopener">Get Ollama</a></p>

        <h2>3. LM Studio</h2>
        <p><strong>Best for:</strong> People who want a polished, graphical interface and an experience similar to an
            app store.</p>
        <p>LM Studio provides a beautiful interface. You simply search for a model, click download, and switch to the
            chat tab to talk to it or start the local server for LocalWriter to use. It handles all the complex setup
            behind the scenes.</p>
        <p>ðŸ‘‰ <a href="https://lmstudio.ai" target="_blank" rel="noopener">Get LM Studio</a></p>

        <hr>

        <h3>Linking them to LocalWriter</h3>
        <p>Once you have one of these tools running an AI model, they provide a <strong>local endpoint URL</strong>
            (typically something like <code>http://localhost:11434</code> for Ollama or
            <code>http://localhost:1234</code> for LM Studio).
        </p>
        <p>In LibreOffice, open <strong>LocalWriter &rarr; Settings</strong> and paste in that local endpoint URL.
            You're now editing entirely offline!</p>
    </div>
</section>
{% endblock %}