from __future__ import annotations

"""
Shared evaluation helpers for the Writer DSPy program.

This centralizes correctness / token accounting so both run_eval.py and
multi-model scripts can reuse the same logic.
"""

from dataclasses import dataclass
from typing import Any, Iterable, List, Tuple

import dspy

from dataset import to_dspy_examples
from metric import TOKEN_PENALTY_LAMBDA
from metric import TOKEN_PENALTY_LAMBDA
from program import build_program


class JudgeSignature(dspy.Signature):
    """
    Judge the quality of a model's generated document compared to a task and optionally a gold standard.
    """
    document_content = dspy.InputField(desc="The input document content.")
    user_question = dspy.InputField(desc="The user's instructions/question.")
    model_answer = dspy.InputField(desc="The document generated by the model.")
    gold_answer = dspy.InputField(desc="A gold standard/perfect version of the document (optional).")
    rubric = dspy.InputField(desc="Specific scoring criteria for this task (optional).")
    task_category = dspy.InputField(desc="The category of the task: 'structural' (data, layout) or 'creative' (tone, rewriting).")

    thought_process = dspy.OutputField(desc="Step-by-step reasoning about the quality across dimensions.")
    accuracy_score = dspy.OutputField(desc="Score 1-5: How well it follows instructions and captures meaning.")
    formatting_score = dspy.OutputField(desc="Score 1-5: Layout, HTML/CSS quality, and structure fidelity.")
    naturalness_score = dspy.OutputField(desc="Score 1-5: Tone, conciseness, and professional quality.")


class JudgeModule(dspy.Module):
    def __init__(self):
        super().__init__()
        self.judge = dspy.Predict(JudgeSignature)

    def forward(self, document_content, user_question, model_answer, gold_answer=None, rubric=None, task_category="structural"):
        pred = self.judge(
            document_content=document_content,
            user_question=user_question,
            model_answer=model_answer,
            gold_answer=gold_answer or "N/A",
            rubric=rubric or "N/A",
            task_category=task_category
        )
        
        # Calculate weighted score (0.0 to 1.0)
        try:
            acc = float(pred.accuracy_score)
            fmt = float(pred.formatting_score)
            
            if task_category == "creative":
                # Creative: Accuracy 30%, Formatting 20%, Naturalness 50%
                nat = float(pred.naturalness_score)
                weighted = (acc * 0.3 + fmt * 0.2 + nat * 0.5) / 5.0
            else:
                # Structural: Accuracy 60%, Formatting 40%, Naturalness 0%
                # Explicitly set output to N/A for logging clarity
                pred.naturalness_score = "N/A"
                weighted = (acc * 0.6 + fmt * 0.4) / 5.0
                
            pred.score = min(max(weighted, 0.0), 1.0)
        except (ValueError, TypeError, AttributeError):
            pred.score = 0.0
            
        return pred


@dataclass
class ExampleEval:
    """Per-example evaluation result."""

    task_id: str
    correctness: float
    missing_expected: list[str]
    found_reject: list[str]
    metric_score: float
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    final_document: str
    judge_score: float | None = None
    judge_reasoning: str | None = None
    judge_accuracy: float | None = None
    judge_formatting: float | None = None
    judge_naturalness: float | None = None
    task_category: str | None = None
    gold_document: str | None = None
    error: str | None = None


def _correctness_breakdown(example: Any, final_document: str) -> Tuple[float, list[str], list[str]]:
    """Return (score, list of missing expected, list of bad reject found)."""
    expected = getattr(example, "expected_contains", []) or []
    reject = getattr(example, "reject_contains", []) or []
    score = 1.0
    missing: list[str] = []
    for s in expected:
        if s not in (final_document or ""):
            score -= 0.2
            missing.append(s)
    found_reject: list[str] = []
    for s in reject:
        if s in (final_document or ""):
            score -= 0.3
            found_reject.append(s)
    # Clamp to [0, 1]
    score = max(0.0, min(1.0, score))
    return score, missing, found_reject


def _get_tokens_from_pred(pred: Any, debug_usage: bool = False) -> Tuple[int, int, int]:
    """
    Extract (prompt_tokens, completion_tokens, total_tokens) from a DSPy prediction.

    Handles OpenRouter / LiteLLM-style get_lm_usage() payloads.
    """
    prompt_tokens = 0
    completion_tokens = 0
    total_tokens = 0
    try:
        usage = pred.get_lm_usage()
        if not usage or not isinstance(usage, dict):
            if debug_usage:
                print(f"  [debug] get_lm_usage() = {usage!r}", flush=True)
            return 0, 0, 0
        # Usage is a dict keyed by model; take the first entry with token info.
        for model_data in usage.values():
            if not isinstance(model_data, dict):
                continue
            if "total_tokens" in model_data:
                total_tokens = int(model_data["total_tokens"])
                # Best-effort split if available.
                prompt_tokens = int(
                    model_data.get("prompt_tokens")
                    or model_data.get("input_tokens")
                    or 0
                )
                completion_tokens = int(
                    model_data.get("completion_tokens")
                    or model_data.get("output_tokens")
                    or 0
                )
                return prompt_tokens, completion_tokens, total_tokens
            prompt_tokens = int(
                model_data.get("prompt_tokens")
                or model_data.get("input_tokens")
                or 0
            )
            completion_tokens = int(
                model_data.get("completion_tokens")
                or model_data.get("output_tokens")
                or 0
            )
            if prompt_tokens or completion_tokens:
                total_tokens = prompt_tokens + completion_tokens
                return prompt_tokens, completion_tokens, total_tokens
        if debug_usage:
            print(f"  [debug] get_lm_usage() = {usage!r} (no token keys found)", flush=True)
    except Exception as e:  # pragma: no cover - defensive
        if debug_usage:
            print(f"  [debug] get_lm_usage error: {e}", flush=True)
    return 0, 0, 0


def run_eval_on_examples(
    program: Any,
    examples: Iterable[Any],
    *,
    verbose: bool = False,
    debug_usage: bool = False,
    bust_cache: bool = True,
    quiet: bool = False,
    judge_lm: dspy.LM | None = None,
    gold_lm: dspy.LM | None = None,
) -> List[ExampleEval]:
    """
    Run the Writer program on a sequence of examples and return per-example results.

    - program: WriterAssistant instance built via build_program().
    - examples: iterable of dspy.Example with fields document_content, user_question.
    - bust_cache: when True, appends a unique suffix to the instruction per example
      (via build_program) to avoid OpenRouter's prompt cache interfering with token
      accounting.
    - quiet: when True, no per-example prints (e.g. when running multiple models in parallel).
    - judge_lm: optional DSPy LM to use as a judge.
    - gold_lm: optional DSPy LM to use to generate/update gold standard documents.
    """
    results: list[ExampleEval] = []
    examples = list(examples)
    n = len(examples)
    base_instruction = getattr(program, "instruction", None) or ""

    judge_mod = JudgeModule() if judge_lm else None

    # We need to track total tokens including judge/gold if we want complete cost,
    # but the current infra tracks usage per-prediction.
    with dspy.settings.context(track_usage=True, cache=False):
        for i, ex in enumerate(examples):
            task_id = getattr(ex, "task_id", "") or f"example_{i}"
            category = getattr(ex, "category", "structural")
            doc = getattr(ex, "document_content", "")
            question = getattr(ex, "user_question", "")
            rubric = getattr(ex, "rubric", "")
            gold = getattr(ex, "gold_document", "")

            if not quiet:
                print(f"--- [{i+1}/{n}] {task_id} ---")
                print(f"  Q: {question[:80]}{'...' if len(question) > 80 else ''}")
                print("  Calling model (may take 15â€“60s)...", flush=True)

            # 1. Gold standard generation (if requested and missing)
            if gold_lm and not gold:
                if not quiet:
                    print(f"  Generating gold standard with {gold_lm.model}...")
                with dspy.settings.context(lm=gold_lm):
                    # We reuse the program to see what a "good" model does, 
                    # but we could also use a separate "GoldGenerator" signature.
                    # For now, let's just use the current program with the gold LM.
                    gold_pred = program(document_content=doc, user_question=question)
                    gold = getattr(gold_pred, "final_document", "") or ""
                    if not quiet:
                        print(f"  Gold generated ({len(gold)} chars).")

            pred = None
            error: str | None = None
            try:
                if bust_cache and base_instruction:
                    # Defer import to avoid circulars when eval_core is imported elsewhere.
                    import uuid

                    cached_suffix = f"\n\n[Eval: {uuid.uuid4().hex[:8]}]"
                    prog = build_program(
                        instruction=base_instruction + cached_suffix,
                        tool_names=None,
                    )
                else:
                    prog = program
                
                pred = prog(document_content=doc, user_question=question)
                final = getattr(pred, "final_document", "") or ""
                
                # Internal correctness (regex/string match)
                correctness, missing, found_reject = _correctness_breakdown(ex, final)
                
                # Judge correctness
                j_score = None
                j_reasoning = None
                
                # Use judge if configured and the task is non-trivial OR has gold standard/rubric
                is_non_trivial_task = getattr(ex, "is_non_trivial", False)
                use_judge = judge_lm and (is_non_trivial_task or gold or rubric)

                if use_judge:
                    if not quiet:
                        print("  Calling judge...", flush=True)
                    with dspy.settings.context(lm=judge_lm):
                        j_result = judge_mod(
                            document_content=doc,
                            user_question=question,
                            model_answer=final,
                            gold_answer=gold,
                            rubric=rubric,
                            task_category=category
                        )
                        j_score = float(j_result.score)
                        j_reasoning = j_result.thought_process
                        j_accuracy = getattr(j_result, "accuracy_score", None)
                        j_formatting = getattr(j_result, "formatting_score", None)
                        j_naturalness = getattr(j_result, "naturalness_score", None)
                        
                        if not quiet:
                            print(f"  judge_score={j_score:.2f} [{category}] (Acc:{j_accuracy} Fmt:{j_formatting} Nat:{j_naturalness})")
                            print(f"  judge_reasoning: {j_reasoning}")
                    
                    # Merge scores: prioritize judge if it exists, otherwise use string matching
                    # User asked for "nuanced score", so judge score is the primary metric now.
                    effective_correctness = j_score
                else:
                    effective_correctness = correctness

                prompt_tok, completion_tok, total_tok = _get_tokens_from_pred(
                    pred, debug_usage=debug_usage
                )
                penalty = TOKEN_PENALTY_LAMBDA * (total_tok / 1000.0)
                metric_score = max(0.0, effective_correctness - penalty)
                snippet = (final[:300] + "...") if len(final) > 300 else final
                
                if not quiet:
                    print(
                        f"  correctness={effective_correctness:.2f}  tokens={total_tok}  score={metric_score:.3f}"
                    )
                    print(f"  doc snippet: {snippet!r}")
                
                    results.append(
                        ExampleEval(
                            task_id=task_id,
                            correctness=effective_correctness,
                            missing_expected=missing,
                            found_reject=found_reject,
                            metric_score=metric_score,
                            prompt_tokens=prompt_tok,
                            completion_tokens=completion_tok,
                            total_tokens=total_tok,
                            final_document=final,
                            judge_score=j_score,
                            judge_reasoning=j_reasoning,
                            judge_accuracy=float(j_accuracy) if j_accuracy and j_accuracy != "N/A" else None,
                            judge_formatting=float(j_formatting) if j_formatting and j_formatting != "N/A" else None,
                            judge_naturalness=float(j_naturalness) if j_naturalness and j_naturalness != "N/A" else None,
                            task_category=category,
                            gold_document=gold,
                            error=None,
                        )
                    )
            except Exception as e:  # pragma: no cover - keep eval robust
                error = str(e)
                if not quiet:
                    print(f"  ERROR: {error}")
                results.append(
                    ExampleEval(
                        task_id=task_id,
                        correctness=0.0,
                        missing_expected=[],
                        found_reject=[],
                        metric_score=0.0,
                        prompt_tokens=0,
                        completion_tokens=0,
                        total_tokens=0,
                        final_document="",
                        error=error,
                    )
                )
            if not quiet:
                print()
    return results


def summarize_results(results: Iterable[ExampleEval]) -> dict:
    """Compute simple aggregates over a list of ExampleEval objects."""
    results = list(results)
    if not results:
        return {
            "avg_correctness": 0.0,
            "avg_metric_score": 0.0,
            "total_tokens": 0,
        }
    n = len(results)
    avg_correctness = sum(r.correctness for r in results) / n
    avg_metric = sum(r.metric_score for r in results) / n
    total_tokens = sum(r.total_tokens for r in results)
    return {
        "avg_correctness": avg_correctness,
        "avg_metric_score": avg_metric,
        "total_tokens": total_tokens,
    }

